{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot find .env file\n",
      "/data03/home/gary.wei/projects/evagpt/notebooks\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "%cd {os.getenv(\"PROJECT_PATH\") or \".\"}\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from absl import logging\n",
    "from tqdm.notebook import tqdm\n",
    "from timeit import default_timer as timer\n",
    "import pickle\n",
    "\n",
    "logging.set_verbosity(logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandarallel import pandarallel\n",
    "\n",
    "pandarallel.initialize(\n",
    "    nb_workers=os.cpu_count(),\n",
    "    progress_bar=True,\n",
    "    verbose=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_df(df: pd.DataFrame):\n",
    "    display(df.head())\n",
    "    print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "import multiprocessing as mp\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "def get_datasets(\n",
    "    tokenizer: PreTrainedTokenizerBase,\n",
    "    dataset_path: str,\n",
    "    dataset_name: str,\n",
    "    block_size: int = 1024,\n",
    "):\n",
    "    def preprocess_function(examples):\n",
    "        tokenized = tokenizer([s + \"\\n\\n\" for s in examples[\"text\"]])\n",
    "\n",
    "        return tokenized\n",
    "\n",
    "    def group_texts(examples):\n",
    "        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "        total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "        result = {\n",
    "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "        return result\n",
    "\n",
    "    dataset_raw = load_dataset(dataset_path, dataset_name)\n",
    "    dataset = dataset_raw.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=mp.cpu_count(),\n",
    "        load_from_cache_file=True,\n",
    "        remove_columns=\"text\",\n",
    "        desc=f\"Tokenizing {dataset_name} dataset\",\n",
    "    ).map(\n",
    "        group_texts,\n",
    "        batched=True,\n",
    "        num_proc=mp.cpu_count(),\n",
    "        load_from_cache_file=True,\n",
    "        desc=f\"Grouping texts in chunks of {block_size}\",\n",
    "    )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee1971ed80b448b944f7292acd1d167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing wikitext-2-raw-v1 dataset (num_proc=128):   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b814832cccb840fa843bc5de66ff6be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing wikitext-2-raw-v1 dataset (num_proc=128):   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb3fa58b731f4995a84000ee59e18b07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing wikitext-2-raw-v1 dataset (num_proc=128):   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b04bf34775d94e70ab173cd06ea90a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 1024 (num_proc=128):   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "225e81d0ad8e450eae9af1241586a00d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 1024 (num_proc=128):   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4ad2ef3e75e423d800b8f388ed2c6ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 1024 (num_proc=128):   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)\n",
    "\n",
    "datasets = get_datasets(\n",
    "    tokenizer, \"wikitext\", \"wikitext-2-raw-v1\", block_size=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "import jax.random as jrandom\n",
    "\n",
    "def data_loader(rng: jrandom.PRNGKey, dataset: Dataset, batch_size: int, shuffle: bool = False, drop_last=True):\n",
    "    \"\"\"\n",
    "    Returns batches of size `batch_size` from `dataset`. If `drop_last` is set to `False`, the final batch may be incomplete,\n",
    "    and range in size from 1 to `batch_size`. Shuffle batches if `shuffle` is `True`.\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        batch_idx = jrandom.permutation(rng, len(dataset))\n",
    "        batch_idx = np.asarray(batch_idx)\n",
    "    else:\n",
    "        batch_idx = np.arange(len(dataset))\n",
    "\n",
    "    if drop_last:\n",
    "        steps_per_epoch = len(dataset) // batch_size\n",
    "        batch_idx = batch_idx[: steps_per_epoch * batch_size]  # Skip incomplete batch.\n",
    "        batch_idx = batch_idx.reshape((steps_per_epoch, batch_size))\n",
    "    else:\n",
    "        steps_per_epoch = math.ceil(len(dataset) / batch_size)\n",
    "        batch_idx = np.array_split(batch_idx, steps_per_epoch)\n",
    "\n",
    "    for idx in batch_idx:\n",
    "        batch = dataset[idx]\n",
    "        batch = {k: np.array(v) for k, v in batch.items()}\n",
    "\n",
    "        yield batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': Array([ 628,  796,  569, ..., 1998, 2173,  389], dtype=int32),\n",
       " 'attention_mask': Array([1, 1, 1, ..., 1, 1, 1], dtype=int32),\n",
       " 'labels': Array([ 628,  796,  569, ..., 1998, 2173,  389], dtype=int32)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train = datasets[\"train\"].with_format(\"jax\")\n",
    "\n",
    "ds_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "====================\n",
      "<class 'dict'>\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(8, 1024)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(8, 1024)\n",
      "Epoch 2\n",
      "====================\n",
      "<class 'dict'>\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(8, 1024)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(8, 1024)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from flax.jax_utils import prefetch_to_device\n",
    "from jax.debug import visualize_array_sharding\n",
    "\n",
    "tf_ds_train = (\n",
    "    datasets[\"train\"]\n",
    "    .select(range(40))\n",
    "    .to_tf_dataset(\n",
    "        batch_size=8,\n",
    "        columns=[\"input_ids\", \"attention_mask\"],\n",
    "        label_cols=\"labels\",\n",
    "        drop_remainder=True,\n",
    "    )\n",
    "    .shuffle(len(datasets), reshuffle_each_iteration=True)\n",
    "    .repeat()\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    "    .as_numpy_iterator()\n",
    ")\n",
    "\n",
    "gpu_iter = prefetch_to_device(tf_ds_train, 4)\n",
    "\n",
    "for epoch in range(2):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    for x, y in gpu_iter:\n",
    "        print(\"=\" * 20)\n",
    "        print(type(x))\n",
    "        print(type(x[\"input_ids\"]))\n",
    "        print(x[\"input_ids\"].shape)\n",
    "        print(type(y))\n",
    "        print(y.shape)\n",
    "        # # print(type(batch[0][\"input_ids\"]))\n",
    "        # # print(type(batch[1]))\n",
    "        # print(x[\"input_ids\"][0, :5])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
