{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aris/projects/evagpt\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "%cd {os.getenv(\"PROJECT_PATH\") or \".\"}\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from absl import logging\n",
    "from tqdm.notebook import tqdm\n",
    "from timeit import default_timer as timer\n",
    "import pickle\n",
    "\n",
    "logging.set_verbosity(logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandarallel import pandarallel\n",
    "\n",
    "pandarallel.initialize(\n",
    "    nb_workers=os.cpu_count(),\n",
    "    progress_bar=True,\n",
    "    verbose=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_df(df: pd.DataFrame):\n",
    "    display(df.head())\n",
    "    print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "import multiprocessing as mp\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "def get_datasets(\n",
    "    tokenizer: PreTrainedTokenizerBase,\n",
    "    dataset_path: str,\n",
    "    dataset_name: str,\n",
    "    block_size: int = 1024,\n",
    "):\n",
    "    def preprocess_function(examples):\n",
    "        tokenized = tokenizer([s + \"\\n\\n\" for s in examples[\"text\"]])\n",
    "\n",
    "        return tokenized\n",
    "\n",
    "    def group_texts(examples):\n",
    "        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "        total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "        result = {\n",
    "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "        return result\n",
    "\n",
    "    dataset_raw = load_dataset(dataset_path, dataset_name)\n",
    "    dataset = dataset_raw.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=mp.cpu_count(),\n",
    "        load_from_cache_file=True,\n",
    "        remove_columns=\"text\",\n",
    "        desc=f\"Tokenizing {dataset_name} dataset\",\n",
    "    ).map(\n",
    "        group_texts,\n",
    "        batched=True,\n",
    "        num_proc=mp.cpu_count(),\n",
    "        load_from_cache_file=True,\n",
    "        desc=f\"Grouping texts in chunks of {block_size}\",\n",
    "    )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)\n",
    "\n",
    "datasets = get_datasets(\n",
    "    tokenizer, \"wikitext\", \"wikitext-2-raw-v1\", block_size=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "import jax.random as jrandom\n",
    "\n",
    "def data_loader(rng: jrandom.PRNGKey, dataset: Dataset, batch_size: int, shuffle: bool = False, drop_last=True):\n",
    "    \"\"\"\n",
    "    Returns batches of size `batch_size` from `dataset`. If `drop_last` is set to `False`, the final batch may be incomplete,\n",
    "    and range in size from 1 to `batch_size`. Shuffle batches if `shuffle` is `True`.\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        batch_idx = jrandom.permutation(rng, len(dataset))\n",
    "        batch_idx = np.asarray(batch_idx)\n",
    "    else:\n",
    "        batch_idx = np.arange(len(dataset))\n",
    "\n",
    "    if drop_last:\n",
    "        steps_per_epoch = len(dataset) // batch_size\n",
    "        batch_idx = batch_idx[: steps_per_epoch * batch_size]  # Skip incomplete batch.\n",
    "        batch_idx = batch_idx.reshape((steps_per_epoch, batch_size))\n",
    "    else:\n",
    "        steps_per_epoch = math.ceil(len(dataset) / batch_size)\n",
    "        batch_idx = np.array_split(batch_idx, steps_per_epoch)\n",
    "\n",
    "    for idx in batch_idx:\n",
    "        batch = dataset[idx]\n",
    "        batch = {k: np.array(v) for k, v in batch.items()}\n",
    "\n",
    "        yield batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([ 628,  796,  569, ..., 1998, 2173,  389]),\n",
       " 'attention_mask': array([1, 1, 1, ..., 1, 1, 1]),\n",
       " 'labels': array([ 628,  796,  569, ..., 1998, 2173,  389])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flax.training import common_utils, jax_utils\n",
    "\n",
    "lm_ds = datasets.with_format(\"jax\")\n",
    "lm_train_dataset = lm_ds[\"train\"]\n",
    "\n",
    "for epoch in range(10):\n",
    "    ds_epoch = lm_train_dataset.shuffle(seed=epoch)\n",
    "\n",
    "    def data_stream():\n",
    "        for batch in ds_epoch:\n",
    "            yield common_utils.shard(batch)\n",
    "            \n",
    "    train_iter = jax_utils.prefetch_to_device("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evagpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
